{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0djWRt6puAl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from google.colab import drive\n",
        "\n",
        "def load_data_from_folder(folder_path, image_size):\n",
        "    \"\"\"Loads images and labels from a folder containing subdirectories for each class.\"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    classes = []\n",
        "\n",
        "    for idx, class_dir in enumerate(os.listdir(folder_path)):\n",
        "        class_path = os.path.join(folder_path, class_dir)\n",
        "        if os.path.isdir(class_path):\n",
        "            classes.append(class_dir)\n",
        "            for img_file in os.listdir(class_path):\n",
        "                img_path = os.path.join(class_path, img_file)\n",
        "                try:\n",
        "                    img = tf.keras.preprocessing.image.load_img(img_path, target_size=image_size)\n",
        "                    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "                    data.append(img_array)\n",
        "                    labels.append(idx)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "    data = np.array(data, dtype='float32') / 255.0\n",
        "    labels = np.array(labels)\n",
        "    return data, labels, classes\n",
        "\n",
        "def extract_features(model, data):\n",
        "    \"\"\"Extracts features using a pre-trained model.\"\"\"\n",
        "    features = model.predict(data, batch_size=32, verbose=1)\n",
        "    return features\n",
        "\n",
        "def create_generators(data, labels, batch_size):\n",
        "    \"\"\"Creates shuffled training and validation datasets.\"\"\"\n",
        "    data, labels = shuffle(data, labels, random_state=42)\n",
        "    split_idx = int(0.8 * len(data))\n",
        "\n",
        "    train_data, val_data = data[:split_idx], data[split_idx:]\n",
        "    train_labels, val_labels = labels[:split_idx], labels[split_idx:]\n",
        "\n",
        "    train_generator = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).batch(batch_size).shuffle(len(train_data))\n",
        "    val_generator = tf.data.Dataset.from_tensor_slices((val_data, val_labels)).batch(batch_size)\n",
        "\n",
        "    return train_generator, val_generator\n",
        "\n",
        "def build_cnn(input_shape, num_classes):\n",
        "    \"\"\"Builds a CNN model.\"\"\"\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def plot_training_results(history):\n",
        "    \"\"\"Plots training and validation accuracy and loss.\"\"\"\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    dataset_folder = \"/celebrity-face-image-dataset/Celebrity Faces Dataset\"\n",
        "\n",
        "    # Set parameters\n",
        "    image_size = (128, 128)\n",
        "    batch_size = 32\n",
        "\n",
        "    # Load and preprocess data\n",
        "    data, labels, classes = load_data_from_folder(dataset_folder, image_size)\n",
        "\n",
        "    feature_extractor = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "    feature_extractor.trainable = False\n",
        "    features = extract_features(feature_extractor, data)\n",
        "    features = features.reshape(features.shape[0], -1)\n",
        "    train_generator, val_generator = create_generators(features, labels, batch_size)\n",
        "    input_shape = (features.shape[1],)\n",
        "    num_classes = len(classes)\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=input_shape),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    epochs = 20\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        validation_data=val_generator,\n",
        "        epochs=epochs\n",
        "    )\n",
        "\n",
        "    print(\"Class Indices:\", {i: c for i, c in enumerate(classes)})\n",
        "    plot_training_results(history)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}